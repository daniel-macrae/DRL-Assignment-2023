{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 5, stride=3)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(576, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, number_of_actions)\n",
    "\n",
    "        # mat1 and mat2 shapes cannot be multiplied (8x1024 and 16x512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #print('doing conv3')\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A model that is tiny, but runs much faster (for testing the code without waiting to train a good model)\n",
    "class speedyDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(speedyDQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 1, 3, stride=10)   # modify input shape to match your input data size\n",
    "        # fully connected layers\n",
    "        self.fc2 = torch.nn.Linear(16, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = F.max_pool2d(x, (2, 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, num_moves, eps_start, eps_min, eps_decay, memory, batch_size, learning_rate, amsgrad, gamma, target_network_update_rate):\n",
    "        self.gamma = gamma\n",
    "        #self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = memory\n",
    "        self.num_possible_moves = num_moves\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_decay = eps_decay\n",
    "\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.target_network_update_rate = target_network_update_rate\n",
    "\n",
    "        self.model = DQN(self.num_possible_moves).to(device)\n",
    "        self.target_network = DQN(self.num_possible_moves).to(device)\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, amsgrad=amsgrad)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, testing = False): \n",
    "        # the 'testing' variable is a way for us to enforce that the agent is exploiting (not exploring) during the 10 episodes of testing\n",
    "        if np.random.rand() <= self.epsilon and testing == False:\n",
    "            return torch.tensor([[random.randrange(self.num_possible_moves)]], device=device, dtype=torch.long)\n",
    "\n",
    "        q_values = self.model(state)\n",
    "        action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        return action # returns a tensor of shape [[n]] (where n is the action number)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        self.steps += 1\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # use of masking to handle the final states (where there is no next state)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        # concatenate the states, actions and rewards into batches\n",
    "        batch_of_states = torch.cat(batch.state) \n",
    "        batch_of_actions = torch.cat(batch.action)\n",
    "        batch_of_rewards = torch.cat(batch.reward)\n",
    "\n",
    "        # get the Q(s_t, a) values for the current state and the chosen action\n",
    "        state_action_values = self.model(batch_of_states).gather(1, batch_of_actions)\n",
    "\n",
    "        # Compute state-action values for all next states using the target network:  max(Q(s_{t+1}, a)).\n",
    "        # 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]  # get the max Q value\n",
    "        \n",
    "        # set the temporal difference learning target\n",
    "        TD_targets = (batch_of_rewards + (self.gamma * next_state_values)  ).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        TD_loss = criterion(state_action_values, TD_targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        TD_loss.backward()\n",
    "        \n",
    "        # clip the losses (Huber loss)\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        # update the target network every certain number of steps\n",
    "        if self.steps % self.target_network_update_rate == 0:\n",
    "            self.overwrite_target_network()\n",
    "            \n",
    "\n",
    "        self.update_eps()\n",
    "\n",
    "        del non_final_mask, non_final_next_states, batch_of_states, batch_of_actions, batch_of_rewards, state_action_values, next_state_values, TD_targets\n",
    "\n",
    "\n",
    "    def update_eps(self):\n",
    "        if self.epsilon > self.eps_min:\n",
    "            self.epsilon = self.eps_start * np.exp(-self.episode/self.eps_decay)\n",
    "            # keep the epsilon value from going below the minimum\n",
    "            if self.epsilon < self.eps_min: \n",
    "                self.epsilon = self.eps_min\n",
    "\n",
    "    # update the target network by overwriting it with the current model\n",
    "    def overwrite_target_network(self):\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH\n",
    "some parameters to look at in the first box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = [32, 128]\n",
    "GAMMA = [0.8, 0.99]\n",
    "EPS_START = [0.8, 1]\n",
    "EPS_END = [0, 0.05, 0.1]\n",
    "EPS_DECAY = [1000, 2000, 3000]\n",
    "LR = [1e-3, 1e-4]\n",
    "MEMORYBUFFER = [1000, 5000]\n",
    "AMSGRAD = [True, False]\n",
    "TARGETNET_UPDATE_RATE = [10, 50]\n",
    "\n",
    "\n",
    "hyper_grid = {'batch_size' : BATCH_SIZE,\n",
    "              'gamma' : GAMMA,\n",
    "              'eps_start' : EPS_START,\n",
    "              'eps_end' : EPS_END,\n",
    "              'eps_decay' : EPS_DECAY,\n",
    "              'learning_rate' : LR,\n",
    "              'memory_buffer' : MEMORYBUFFER,\n",
    "              'ams_grad' : AMSGRAD,\n",
    "              'targetnet_update_rate' : TARGETNET_UPDATE_RATE}\n",
    "\n",
    "grid = list(ParameterGrid(hyper_grid))\n",
    "random.shuffle(grid)  # randomly shuffle the grid (in case we don't get many trials done, at least there is more variety)\n",
    "print(len(grid))\n",
    "\n",
    "\n",
    "\n",
    "# AMOUNT OF GRID TO SAMPLE\n",
    "gridSampleSize = 0.6\n",
    "\n",
    "\n",
    "# stuffs\n",
    "DQN_model = 1   # in case we design more models, we'll call the one we have now the 1st one\n",
    "output_filename = \"Grid_search_0.xlsx\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 5000\n",
    "else:\n",
    "    num_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27544739\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Grid_Search.ipynb Cell 9\u001b[0m in \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     state \u001b[39m=\u001b[39m next_state\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m# optimise the DQN model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     agent\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# testing of the agent between 10 episode blocks\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m episode \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m# store the rewards of the last 10 training episode (NO SEPERATE TESTING HERE, SAVES SOME RUNNING TIME)\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Grid_Search.ipynb Cell 9\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m TD_loss \u001b[39m=\u001b[39m criterion(state_action_values, TD_targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# Optimize the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m TD_loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X13sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m# clip the losses (Huber loss)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\handwritingRecognition\\Lib\\site-packages\\torch\\optim\\optimizer.py:456\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mif\u001b[39;00m foreach:\n\u001b[0;32m    455\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[1;32m--> 456\u001b[0m \u001b[39mwith\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49mrecord_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_zero_grad_profile_name):\n\u001b[0;32m    457\u001b[0m     \u001b[39mfor\u001b[39;49;00m group \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_groups:\n\u001b[0;32m    458\u001b[0m         \u001b[39mfor\u001b[39;49;00m p \u001b[39min\u001b[39;49;00m group[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\handwritingRecognition\\Lib\\site-packages\\torch\\autograd\\profiler.py:507\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m    506\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[1;32m--> 507\u001b[0m         torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_exit\u001b[39m.\u001b[39;49m_RecordFunction(record)\n\u001b[0;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\handwritingRecognition\\Lib\\site-packages\\torch\\_ops.py:287\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GRID SEARCH LOOP\n",
    "\n",
    "env = CatchEnv()\n",
    "num_moves = env.get_num_actions()\n",
    "\n",
    "\n",
    "RESULTS_DATAFRAME = pd.DataFrame(columns=[\"DQN_model\",'batch_size', 'gamma', 'eps_start', 'eps_end', 'eps_decay', 'learning_rate', 'memory_buffer', 'ams_grad', 'targetnet_update_rate',\n",
    "                                          \"avgRewards\", \"average_last_100_episodes\", \"best_average_100_episodes\", \"time_of_peak\", \"time_to_convergence\"])\n",
    "\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for params in grid:    \n",
    "\n",
    "    # random sampling...\n",
    "    if random.random() > gridSampleSize:  # samples the grid\n",
    "        continue # skips this set of parameters\n",
    "\n",
    "    # otherwise, go on as normal:\n",
    "    \n",
    "    # make the agent and memory buffer using the parameters\n",
    "    memoryBuffer = MemoryBuffer(params['memory_buffer'])\n",
    "    agent = Agent(num_moves, params['eps_start'], params['eps_end'], params['eps_decay'], memoryBuffer, params['batch_size'], params['learning_rate'], params['ams_grad'], params['gamma'], params['targetnet_update_rate'])\n",
    "\n",
    "    \n",
    "    model_parameters = filter(lambda p: p.requires_grad, agent.model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(params)\n",
    "\n",
    "    # for the results of this episode (and set of parameters)\n",
    "    idx += 1\n",
    "    RewardsList = []\n",
    "    tempReward = []\n",
    "    time_to_convergence = None\n",
    "    best_average = 0\n",
    "    best_episode = None\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        agent.episode += 1\n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0) \n",
    "\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            # agent interacts with the environment\n",
    "            action = agent.select_action(state)    \n",
    "            next_state, reward, terminal = env.step(action.item()) \n",
    "            \n",
    "            # turn everything into tensors here, before putting in memory\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            if not terminal:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            if terminal:\n",
    "                next_state = None\n",
    "                tempReward.append(reward.item())   \n",
    "                            \n",
    "\n",
    "            # add trajectory to memory buffer and move to the next state\n",
    "            agent.memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            # optimise the DQN model\n",
    "            agent.optimize_model()\n",
    "\n",
    "        # testing of the agent between 10 episode blocks\n",
    "        if episode % 10 == 0 and episode > 0:\n",
    "            # store the rewards of the last 10 training episode (NO SEPERATE TESTING HERE, SAVES SOME RUNNING TIME)\n",
    "            RewardsList.append(sum(tempReward)/len(tempReward))\n",
    "            tempReward = []\n",
    "\n",
    "            # find the best average over 100 episodes\n",
    "            running_avg = sum(RewardsList[-10:]) / 10   # each element in RewardsList is an average of 10 episodes\n",
    "            if running_avg > best_average:\n",
    "                best_average = running_avg\n",
    "                best_episode = episode\n",
    "            \n",
    "\n",
    "            # early stopping\n",
    "            # if the average of the previous 100 episodes was above 0.9, we've probably hit convergence so stop (to try and save time)\n",
    "            if episode % 10 == 0 and running_avg > 0.9:\n",
    "                time_to_convergence = episode\n",
    "                break\n",
    "\n",
    "\n",
    "    # store the results in a dataframe, making a new row for this trial here\n",
    "    tempDict = {\"DQN_model\" : DQN_model,\n",
    "                \"avgRewards\" : RewardsList,\n",
    "                \"average_last_100_episodes\" : running_avg,\n",
    "                \"best_average_100_episodes\" : best_average,\n",
    "                \"time_of_peak\" : best_episode,\n",
    "                \"time_to_convergence\" : time_to_convergence}\n",
    "    \n",
    "    resultsDict = {**params.copy(), **tempDict}  # make a line for in the results dict\n",
    "\n",
    "    RESULTS_DATAFRAME.loc[idx] = resultsDict\n",
    "\n",
    "    RESULTS_DATAFRAME.to_excel(output_filename) # saves on every iteration (in case this takes long, or crashes, we can still pull the results out)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
