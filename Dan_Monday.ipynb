{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\miniconda3\\envs\\footballtracking\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "For stuff like plotting the rewards during the training\n",
    "And for doing the 10 episodes of evaluation seperately from the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# plots average rewards over the episodes, in a way that updates the plot during the training loop\n",
    "def learningPlotter(episodeNumber, avgReward, show_result=False):\n",
    "    plt.figure(1)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.ylabel('Average reward (over 10 episodes)')\n",
    "    plt.xlabel('Episode number')\n",
    "    plt.plot(episodeNumber, avgReward)\n",
    "    # Take 100 episode averages and plot them too\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does 10 episodes of evaluating the agent; returns the average reward over these 10 episodes\n",
    "# the agent does not learn, nor store anything in memory, as it is purely exploiting the environment\n",
    "\n",
    "# the loop is very similar to the training loop code below\n",
    "def testingTenEpisodes(agent, env):\n",
    "    total_reward = 0\n",
    "    for episode in range(10):        \n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        state = state.permute(2, 0, 1).unsqueeze(0) \n",
    "\n",
    "        terminal = False\n",
    "        \n",
    "        while not terminal:\n",
    "            action = agent.select_action(state, testing = True)     # testing=True enforces exploitation\n",
    "            next_state, reward, terminal = env.step(action.item()) \n",
    "\n",
    "            if not terminal:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "                next_state = next_state.permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            if terminal:\n",
    "                next_state = None\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    return total_reward / 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# my attempts to tune things\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 5, stride=3)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(576, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, number_of_actions)\n",
    "\n",
    "        # mat1 and mat2 shapes cannot be multiplied (8x1024 and 16x512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #print('doing conv3')\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A model that is tiny, but runs much faster (for testing the code without waiting to train a good model)\n",
    "class speedyDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(speedyDQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 1, 3, stride=10)   # modify input shape to match your input data size\n",
    "        # fully connected layers\n",
    "        self.fc2 = torch.nn.Linear(16, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, num_moves, eps_start, eps_min, eps_decay, memory, batch_size, optimizer, gamma):\n",
    "        self.gamma = gamma\n",
    "        #self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = memory\n",
    "        self.num_possible_moves = num_moves\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_decay = eps_decay\n",
    "\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.target_network_update_rate = 10\n",
    "\n",
    "        self.model = DQN(self.num_possible_moves).to(device)\n",
    "        self.target_network = DQN(self.num_possible_moves).to(device)\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, testing = False): \n",
    "        # the 'testing' variable is a way for us to enforce that the agent is exploiting (not exploring) during the 10 episodes of testing\n",
    "        if np.random.rand() <= self.epsilon and testing == False:\n",
    "            return torch.tensor([[random.randrange(self.num_possible_moves)]], device=device, dtype=torch.long)\n",
    "\n",
    "        q_values = self.model(state)\n",
    "        action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        return action # returns a tensor of shape [[n]] (where n is the action number)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        self.steps += 1\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # use of masking to handle the final states (where there is no next state)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        # concatenate the states, actions and rewards into batches\n",
    "        batch_of_states = torch.cat(batch.state) \n",
    "        batch_of_actions = torch.cat(batch.action)\n",
    "        batch_of_rewards = torch.cat(batch.reward)\n",
    "\n",
    "        # get the Q(s_t, a) values for the current state and the chosen action\n",
    "        state_action_values = self.model(batch_of_states).gather(1, batch_of_actions)\n",
    "\n",
    "        # Compute state-action values for all next states using the target network:  max(Q(s_{t+1}, a)).\n",
    "        # 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]  # get the max Q value\n",
    "        \n",
    "        # set the temporal difference learning target\n",
    "        TD_targets = (batch_of_rewards + (self.gamma * next_state_values)  ).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        TD_loss = criterion(state_action_values, TD_targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        TD_loss.backward()\n",
    "        \n",
    "        # clip the losses (Huber loss)\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        \"\"\" \n",
    "        how often do we want to replace the target network? \n",
    "        This might be a parameter as well? Or we can do it with TAU and update it slightly on every step,\n",
    "        as in the \"reinforcement_q_learning\" notebook\n",
    "        \"\"\"\n",
    "        # update the target network every certain number of steps\n",
    "        if self.steps % self.target_network_update_rate == 0:\n",
    "            self.overwrite_target_network()\n",
    "            \n",
    "\n",
    "        self.update_eps()\n",
    "\n",
    "\n",
    "    def update_eps(self):\n",
    "        if self.epsilon > self.eps_min:\n",
    "            self.epsilon = self.eps_start * np.exp(-self.episode/self.eps_decay)\n",
    "            # keep the epsilon value from going below the minimum\n",
    "            if self.epsilon < self.eps_min: \n",
    "                self.epsilon = self.eps_min\n",
    "\n",
    "    # update the target network by overwriting it with the current model\n",
    "    def overwrite_target_network(self):\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x233421fb490>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# to see what the decay of epsilon would look like\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1300 # higher value = slower decay\n",
    "\n",
    "episodes = np.linspace(0, 5000, 1000) # 5,000 episodes\n",
    "\n",
    "eps_threshold = EPS_START * np.exp(-episodes / EPS_DECAY)\n",
    "eps_threshold[eps_threshold < EPS_END] = EPS_END\n",
    "\n",
    "plt.plot(episodes, eps_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[0;32m     44\u001b[0m     \u001b[39m# agent interacts with the environment\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action(state)    \n\u001b[1;32m---> 46\u001b[0m     next_state, reward, terminal \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mitem())  \u001b[39m# .item() just uses the value inside of the tensor\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[39m# turn everything into tensors here, before putting in memory\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward], device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Catch.py:54\u001b[0m, in \u001b[0;36mCatchEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     51\u001b[0m terminal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbally \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     52\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mballx \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m) \u001b[39mif\u001b[39;00m terminal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 54\u001b[0m [ \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mappend(resize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage, (\u001b[39m84\u001b[39m, \u001b[39m84\u001b[39m))) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) ]\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps:]\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m]), reward, terminal\n",
      "File \u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Catch.py:54\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     51\u001b[0m terminal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbally \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m4\u001b[39m\n\u001b[0;32m     52\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mballx \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m+\u001b[39m \u001b[39m2\u001b[39m) \u001b[39mif\u001b[39;00m terminal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 54\u001b[0m [ \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mappend(resize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage, (\u001b[39m84\u001b[39;49m, \u001b[39m84\u001b[39;49m))) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) ]\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfps:]\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mtranspose(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m]), reward, terminal\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\footballtracking\\lib\\site-packages\\skimage\\transform\\_warps.py:186\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, output_shape, order, mode, cval, clip, preserve_range, anti_aliasing, anti_aliasing_sigma)\u001b[0m\n\u001b[0;32m    182\u001b[0m     image \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39mgaussian_filter(image, anti_aliasing_sigma,\n\u001b[0;32m    183\u001b[0m                                 cval\u001b[39m=\u001b[39mcval, mode\u001b[39m=\u001b[39mndi_mode)\n\u001b[0;32m    185\u001b[0m zoom_factors \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m factors]\n\u001b[1;32m--> 186\u001b[0m out \u001b[39m=\u001b[39m ndi\u001b[39m.\u001b[39;49mzoom(image, zoom_factors, order\u001b[39m=\u001b[39;49morder, mode\u001b[39m=\u001b[39;49mndi_mode,\n\u001b[0;32m    187\u001b[0m                cval\u001b[39m=\u001b[39;49mcval, grid_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m _clip_warp_output(img_bounds, out, mode, cval, clip)\n\u001b[0;32m    191\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\footballtracking\\lib\\site-packages\\scipy\\ndimage\\_interpolation.py:816\u001b[0m, in \u001b[0;36mzoom\u001b[1;34m(input, zoom, output, order, mode, cval, prefilter, grid_mode)\u001b[0m\n\u001b[0;32m    812\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mdivide(zoom_nominator, zoom_div,\n\u001b[0;32m    813\u001b[0m                     out\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mones_like(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnumpy\u001b[39m.\u001b[39mfloat64),\n\u001b[0;32m    814\u001b[0m                     where\u001b[39m=\u001b[39mzoom_div \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    815\u001b[0m zoom \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mascontiguousarray(zoom)\n\u001b[1;32m--> 816\u001b[0m _nd_image\u001b[39m.\u001b[39;49mzoom_shift(filtered, zoom, \u001b[39mNone\u001b[39;49;00m, output, order, mode, cval, npad,\n\u001b[0;32m    817\u001b[0m                      grid_mode)\n\u001b[0;32m    818\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 5000\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "env = CatchEnv()\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 2000  \n",
    "TAU = 0.005            # <--- we're not using this?\n",
    "LR = 1e-5\n",
    "memoryBuffer = MemoryBuffer(1000)\n",
    "\"\"\" this doesn't get used anyway, its defined inside of the agent \"\"\"\n",
    "#optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "optimizer = None\n",
    "\n",
    "num_moves = env.get_num_actions()\n",
    "\n",
    "agent = Agent(num_moves, EPS_START, EPS_END, EPS_DECAY, memoryBuffer, BATCH_SIZE, optimizer, GAMMA)\n",
    "\n",
    "episodeNumber = []\n",
    "avgReward = []\n",
    "tempReward = []\n",
    "\n",
    "testingRewards = []  # this could be turned into the numpy array that we submit for each result file next week\n",
    "\n",
    "TESTING = True  # if we are doing the evaluation stuff (i.e. for uploading results)\n",
    "PLOTTING = True # if plotting the rewards over episodes\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    agent.episode += 1\n",
    "\n",
    "    state = env.reset()\n",
    "    # get into the dimension [1,4,84,84] by rearranging from [84,84,4] and unsqueezing to add the 1 at the front\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "    state = state.permute(2, 0, 1).unsqueeze(0) \n",
    "\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "        # agent interacts with the environment\n",
    "        action = agent.select_action(state)    \n",
    "        next_state, reward, terminal = env.step(action.item())  # .item() just uses the value inside of the tensor\n",
    "        \n",
    "        # turn everything into tensors here, before putting in memory\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        if not terminal:\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "            next_state = next_state.permute(2, 0, 1).unsqueeze(0)\n",
    "        \n",
    "        if terminal:\n",
    "            next_state = None\n",
    "            tempReward.append(reward.item())\n",
    "\n",
    "            # for plotting rewards and/or testing of the agent\n",
    "            if episode % 10 == 0 and episode > 0:\n",
    "\n",
    "                episodeNumber.append(episode)\n",
    "\n",
    "                if TESTING: # 10 seperate episodes of evaluation (no training takes place)\n",
    "                    score = testingTenEpisodes(agent, env)\n",
    "                    testingRewards.append(score)\n",
    "                    if PLOTTING:\n",
    "                        learningPlotter(episodeNumber, testingRewards)\n",
    "                        \n",
    "                elif PLOTTING: # just visualise the rewards over the last 10 training episodes\n",
    "                    avgReward.append(sum(tempReward)/len(tempReward))\n",
    "                    tempReward = []\n",
    "                    learningPlotter(episodeNumber, avgReward)\n",
    "\n",
    "        # add trajectory to memory buffer\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # move onto the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Sample a batch of transitions from the replay buffer and \n",
    "        # optimise the learned Q function\n",
    "        agent.optimize_model()\n",
    "\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "if TESTING: learningPlotter(episodeNumber, testingRewards, show_result=True)\n",
    "else: learningPlotter(episodeNumber, avgReward, show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
