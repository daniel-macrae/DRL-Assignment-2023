{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 5, stride=3)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(576, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, number_of_actions)\n",
    "\n",
    "        # mat1 and mat2 shapes cannot be multiplied (8x1024 and 16x512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #print('doing conv3')\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A model that is tiny, but runs much faster (for testing the code without waiting to train a good model)\n",
    "class speedyDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(speedyDQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 1, 3, stride=10)   # modify input shape to match your input data size\n",
    "        # fully connected layers\n",
    "        self.fc2 = torch.nn.Linear(16, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = F.max_pool2d(x, (2, 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, num_moves, eps_start, eps_min, eps_decay, memory, batch_size, learning_rate, amsgrad, gamma, target_network_update_rate):\n",
    "        self.gamma = gamma\n",
    "        #self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = memory\n",
    "        self.num_possible_moves = num_moves\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_decay = eps_decay\n",
    "\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.target_network_update_rate = target_network_update_rate\n",
    "\n",
    "        self.model = DQN(self.num_possible_moves).to(device)\n",
    "        self.target_network = DQN(self.num_possible_moves).to(device)\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, amsgrad=amsgrad)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, testing = False): \n",
    "        # the 'testing' variable is a way for us to enforce that the agent is exploiting (not exploring) during the 10 episodes of testing\n",
    "        if np.random.rand() <= self.epsilon and testing == False:\n",
    "            return torch.tensor([[random.randrange(self.num_possible_moves)]], device=device, dtype=torch.long)\n",
    "\n",
    "        q_values = self.model(state)\n",
    "        action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        return action # returns a tensor of shape [[n]] (where n is the action number)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        self.steps += 1\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # use of masking to handle the final states (where there is no next state)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        # concatenate the states, actions and rewards into batches\n",
    "        batch_of_states = torch.cat(batch.state) \n",
    "        batch_of_actions = torch.cat(batch.action)\n",
    "        batch_of_rewards = torch.cat(batch.reward)\n",
    "\n",
    "        # get the Q(s_t, a) values for the current state and the chosen action\n",
    "        state_action_values = self.model(batch_of_states).gather(1, batch_of_actions)\n",
    "\n",
    "        # Compute state-action values for all next states using the target network:  max(Q(s_{t+1}, a)).\n",
    "        # 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]  # get the max Q value\n",
    "        \n",
    "        # set the temporal difference learning target\n",
    "        TD_targets = (batch_of_rewards + (self.gamma * next_state_values)  ).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        TD_loss = criterion(state_action_values, TD_targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        TD_loss.backward()\n",
    "        \n",
    "        # clip the losses (Huber loss)\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        # update the target network every certain number of steps\n",
    "        if self.steps % self.target_network_update_rate == 0:\n",
    "            self.overwrite_target_network()\n",
    "            \n",
    "\n",
    "        self.update_eps()\n",
    "\n",
    "        del non_final_mask, non_final_next_states, batch_of_states, batch_of_actions, batch_of_rewards, state_action_values, next_state_values, TD_targets\n",
    "\n",
    "\n",
    "    def update_eps(self):\n",
    "        if self.epsilon > self.eps_min:\n",
    "            self.epsilon = self.eps_start * np.exp(-self.episode/self.eps_decay)\n",
    "            # keep the epsilon value from going below the minimum\n",
    "            if self.epsilon < self.eps_min: \n",
    "                self.epsilon = self.eps_min\n",
    "\n",
    "    # update the target network by overwriting it with the current model\n",
    "    def overwrite_target_network(self):\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH\n",
    "some parameters to look at in the first box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = [32, 128]\n",
    "GAMMA = [0.8, 0.99]\n",
    "EPS_START = [0.8, 1]\n",
    "EPS_END = [0, 0.05, 0.1]\n",
    "EPS_DECAY = [1000, 2000, 3000]\n",
    "LR = [1e-3, 1e-4]\n",
    "MEMORYBUFFER = [1000, 5000]\n",
    "AMSGRAD = [True, False]\n",
    "TARGETNET_UPDATE_RATE = [10, 50]\n",
    "\n",
    "\n",
    "hyper_grid = {'batch_size' : BATCH_SIZE,\n",
    "              'gamma' : GAMMA,\n",
    "              'eps_start' : EPS_START,\n",
    "              'eps_end' : EPS_END,\n",
    "              'eps_decay' : EPS_DECAY,\n",
    "              'learning_rate' : LR,\n",
    "              'memory_buffer' : MEMORYBUFFER,\n",
    "              'ams_grad' : AMSGRAD,\n",
    "              'targetnet_update_rate' : TARGETNET_UPDATE_RATE}\n",
    "\n",
    "grid = list(ParameterGrid(hyper_grid))\n",
    "random.shuffle(grid)  # randomly shuffle the grid (in case we don't get many trials done, at least there is more variety)\n",
    "print(len(grid))\n",
    "\n",
    "\n",
    "\n",
    "# AMOUNT OF GRID TO SAMPLE\n",
    "gridSampleSize = 0.6\n",
    "\n",
    "\n",
    "# stuffs\n",
    "DQN_model = 1   # in case we design more models, we'll call the one we have now the 1st one\n",
    "output_filename = \"Grid_search_0.xlsx\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 5000\n",
    "else:\n",
    "    num_episodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DQN' object has no attribute 'conv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Grid_Search.ipynb Cell 9\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m terminal \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# agent interacts with the environment\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mselect_action(state)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     next_state, reward, terminal \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action\u001b[39m.\u001b[39mitem()) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# turn everything into tensors here, before putting in memory\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Grid_Search.ipynb Cell 9\u001b[0m in \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrand() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon \u001b[39mand\u001b[39;00m testing \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor([[random\u001b[39m.\u001b[39mrandrange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_possible_moves)]], device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m action \u001b[39m=\u001b[39m q_values\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\handwritingRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\Grid_Search.ipynb Cell 9\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, (\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, (\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#print('doing conv3')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/Grid_Search.ipynb#X11sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#x = F.relu(self.conv3(x))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\handwritingRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DQN' object has no attribute 'conv2'"
     ]
    }
   ],
   "source": [
    "# GRID SEARCH LOOP\n",
    "\n",
    "env = CatchEnv()\n",
    "num_moves = env.get_num_actions()\n",
    "\n",
    "\n",
    "RESULTS_DATAFRAME = pd.DataFrame(columns=[\"DQN_model\",'batch_size', 'gamma', 'eps_start', 'eps_end', 'eps_decay', 'learning_rate', 'memory_buffer', 'ams_grad', 'targetnet_update_rate',\n",
    "                                          \"avgRewards\", \"average_last_100_episodes\", \"best_average_100_episodes\", \"time_of_peak\", \"time_to_convergence\"])\n",
    "\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for params in grid:    \n",
    "\n",
    "    # random sampling...\n",
    "    if random.random() > gridSampleSize:  # samples the grid\n",
    "        continue # skips this set of parameters\n",
    "\n",
    "    # otherwise, go on as normal:\n",
    "    \n",
    "    # make the agent and memory buffer using the parameters\n",
    "    memoryBuffer = MemoryBuffer(params['memory_buffer'])\n",
    "    agent = Agent(num_moves, params['eps_start'], params['eps_end'], params['eps_decay'], memoryBuffer, params['batch_size'], params['learning_rate'], params['ams_grad'], params['gamma'], params['targetnet_update_rate'])\n",
    "\n",
    "    # for the results of this episode (and set of parameters)\n",
    "    idx += 1\n",
    "    RewardsList = []\n",
    "    tempReward = []\n",
    "    time_to_convergence = None\n",
    "    best_average = 0\n",
    "    best_episode = None\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        agent.episode += 1\n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0) \n",
    "\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            # agent interacts with the environment\n",
    "            action = agent.select_action(state)    \n",
    "            next_state, reward, terminal = env.step(action.item()) \n",
    "            \n",
    "            # turn everything into tensors here, before putting in memory\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            if not terminal:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            if terminal:\n",
    "                next_state = None\n",
    "                tempReward.append(reward.item())   \n",
    "                            \n",
    "\n",
    "            # add trajectory to memory buffer and move to the next state\n",
    "            agent.memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            # optimise the DQN model\n",
    "            agent.optimize_model()\n",
    "\n",
    "        # testing of the agent between 10 episode blocks\n",
    "        if episode % 10 == 0 and episode > 0:\n",
    "            # store the rewards of the last 10 training episode (NO SEPERATE TESTING HERE, SAVES SOME RUNNING TIME)\n",
    "            RewardsList.append(sum(tempReward)/len(tempReward))\n",
    "            tempReward = []\n",
    "\n",
    "            # find the best average over 100 episodes\n",
    "            running_avg = sum(RewardsList[-10:]) / 10   # each element in RewardsList is an average of 10 episodes\n",
    "            if running_avg > best_average:\n",
    "                best_average = running_avg\n",
    "                best_episode = episode\n",
    "            \n",
    "\n",
    "            # early stopping\n",
    "            # if the average of the previous 100 episodes was above 0.9, we've probably hit convergence so stop (to try and save time)\n",
    "            if episode % 10 == 0 and running_avg > 0.9:\n",
    "                time_to_convergence = episode\n",
    "                break\n",
    "\n",
    "\n",
    "    # store the results in a dataframe, making a new row for this trial here\n",
    "    tempDict = {\"DQN_model\" : DQN_model,\n",
    "                \"avgRewards\" : RewardsList,\n",
    "                \"average_last_100_episodes\" : running_avg,\n",
    "                \"best_average_100_episodes\" : best_average,\n",
    "                \"time_of_peak\" : best_episode,\n",
    "                \"time_to_convergence\" : time_to_convergence}\n",
    "    \n",
    "    resultsDict = {**params.copy(), **tempDict}  # make a line for in the results dict\n",
    "\n",
    "    RESULTS_DATAFRAME.loc[idx] = resultsDict\n",
    "\n",
    "    RESULTS_DATAFRAME.to_excel(output_filename) # saves on every iteration (in case this takes long, or crashes, we can still pull the results out)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
