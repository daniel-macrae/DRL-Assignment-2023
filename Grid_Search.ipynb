{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\miniconda3\\envs\\footballtracking\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "For stuff like plotting the rewards during the training\n",
    "And for doing the 10 episodes of evaluation seperately from the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does 10 episodes of evaluating the agent; returns the average reward over these 10 episodes\n",
    "# the agent does not learn, nor store anything in memory, as it is purely exploiting the environment\n",
    "\n",
    "# the loop is very similar to the training loop code below\n",
    "def testingTenEpisodes(agent, env):\n",
    "    total_reward = 0\n",
    "    for episode in range(10):        \n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        state = state.permute(2, 0, 1).unsqueeze(0) \n",
    "\n",
    "        terminal = False\n",
    "        \n",
    "        while not terminal:\n",
    "            action = agent.select_action(state, testing = True)     # testing=True enforces exploitation\n",
    "            next_state, reward, terminal = env.step(action.item()) \n",
    "\n",
    "            if not terminal:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "                next_state = next_state.permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            if terminal:\n",
    "                next_state = None\n",
    "\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    return total_reward / 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "# my attempts to tune things\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 5, stride=3)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(576, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, number_of_actions)\n",
    "\n",
    "        # mat1 and mat2 shapes cannot be multiplied (8x1024 and 16x512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #print('doing conv3')\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 5, stride=3)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 3, stride=2)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(576, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, number_of_actions)\n",
    "\n",
    "        # mat1 and mat2 shapes cannot be multiplied (8x1024 and 16x512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #print('doing conv3')\n",
    "        #x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# A model that is tiny, but runs much faster (for testing the code without waiting to train a good model)\n",
    "class speedyDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(speedyDQN, self).__init__()\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 1, 3, stride=10)   # modify input shape to match your input data size\n",
    "        # fully connected layers\n",
    "        self.fc2 = torch.nn.Linear(16, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        #x = F.relu(self.conv2(x))\n",
    "        #x = F.max_pool2d(x, (2, 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, num_moves, eps_start, eps_min, eps_decay, memory, batch_size, learning_rate, amsgrad, gamma, target_network_update_rate):\n",
    "        self.gamma = gamma\n",
    "        #self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = memory\n",
    "        self.num_possible_moves = num_moves\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_decay = eps_decay\n",
    "\n",
    "        self.episode = 0\n",
    "        self.steps = 0\n",
    "        self.target_network_update_rate = target_network_update_rate\n",
    "\n",
    "        self.model = speedyDQN(self.num_possible_moves).to(device)\n",
    "        self.target_network = speedyDQN(self.num_possible_moves).to(device)\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate, amsgrad=amsgrad)\n",
    "\n",
    "    \n",
    "    def select_action(self, state, testing = False): \n",
    "        # the 'testing' variable is a way for us to enforce that the agent is exploiting (not exploring) during the 10 episodes of testing\n",
    "        if np.random.rand() <= self.epsilon and testing == False:\n",
    "            return torch.tensor([[random.randrange(self.num_possible_moves)]], device=device, dtype=torch.long)\n",
    "\n",
    "        q_values = self.model(state)\n",
    "        action = q_values.max(1)[1].view(1, 1)\n",
    "\n",
    "        return action # returns a tensor of shape [[n]] (where n is the action number)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        self.steps += 1\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # use of masking to handle the final states (where there is no next state)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        # concatenate the states, actions and rewards into batches\n",
    "        batch_of_states = torch.cat(batch.state) \n",
    "        batch_of_actions = torch.cat(batch.action)\n",
    "        batch_of_rewards = torch.cat(batch.reward)\n",
    "\n",
    "        # get the Q(s_t, a) values for the current state and the chosen action\n",
    "        state_action_values = self.model(batch_of_states).gather(1, batch_of_actions)\n",
    "\n",
    "        # Compute state-action values for all next states using the target network:  max(Q(s_{t+1}, a)).\n",
    "        # 0 in case the state was final.\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_network(non_final_next_states).max(1)[0]  # get the max Q value\n",
    "        \n",
    "        # set the temporal difference learning target\n",
    "        TD_targets = (batch_of_rewards + (self.gamma * next_state_values)  ).unsqueeze(1)\n",
    "\n",
    "\n",
    "        # Compute the Huber loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        TD_loss = criterion(state_action_values, TD_targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        TD_loss.backward()\n",
    "        \n",
    "        # clip the losses (Huber loss)\n",
    "        torch.nn.utils.clip_grad_value_(self.model.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "        # update the target network every certain number of steps\n",
    "        if self.steps % self.target_network_update_rate == 0:\n",
    "            self.overwrite_target_network()\n",
    "            \n",
    "\n",
    "        self.update_eps()\n",
    "\n",
    "        del non_final_mask, non_final_next_states, batch_of_states, batch_of_actions, batch_of_rewards, state_action_values, next_state_values, TD_targets\n",
    "\n",
    "\n",
    "    def update_eps(self):\n",
    "        if self.epsilon > self.eps_min:\n",
    "            self.epsilon = self.eps_start * np.exp(-self.episode/self.eps_decay)\n",
    "            # keep the epsilon value from going below the minimum\n",
    "            if self.epsilon < self.eps_min: \n",
    "                self.epsilon = self.eps_min\n",
    "\n",
    "    # update the target network by overwriting it with the current model\n",
    "    def overwrite_target_network(self):\n",
    "        self.target_network.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = [32, 128]\n",
    "GAMMA = [0.8, 0.99]\n",
    "EPS_START = [0.8, 1]\n",
    "EPS_END = [0, 0.05, 0.1]\n",
    "EPS_DECAY = [1000, 2000, 3000]\n",
    "LR = [1e-3, 1e-4]\n",
    "MEMORYBUFFER = [1000, 5000]\n",
    "AMSGRAD = [True, False]\n",
    "TARGETNET_UPDATE_RATE = [10, 50]\n",
    "\n",
    "\n",
    "hyper_grid = {'batch_size' : BATCH_SIZE,\n",
    "              'gamma' : GAMMA,\n",
    "              'eps_start' : EPS_START,\n",
    "              'eps_end' : EPS_END,\n",
    "              'eps_decay' : EPS_DECAY,\n",
    "              'learning_rate' : LR,\n",
    "              'memory_buffer' : MEMORYBUFFER,\n",
    "              'ams_grad' : AMSGRAD,\n",
    "              'targetnet_update_rate' : TARGETNET_UPDATE_RATE}\n",
    "\n",
    "grid = list(ParameterGrid(hyper_grid))\n",
    "print(len(grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 5000\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "env = CatchEnv()\n",
    "num_moves = env.get_num_actions()\n",
    "\n",
    "\n",
    "RESULTS_DATAFRAME = pd.DataFrame(columns=[\"DQN_model\",'batch_size', 'gamma', 'eps_start', 'eps_end', 'eps_decay', 'learning_rate', 'memory_buffer', 'ams_grad', 'targetnet_update_rate',\n",
    "                                          \"avgRewards\", \"average_last_100_episodes\", \"best_average_100_episodes\", \"time_of_peak\", \"time_to_convergence\"])\n",
    "\n",
    "DQN_model = 0\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for params in grid[:5]:    \n",
    "\n",
    "    # random sampling...\n",
    "    if random.random() > 0.6:  # samples 60% of the grid\n",
    "        continue # skips this set of parameters\n",
    "\n",
    "    # otherwise, go on as normal:\n",
    "    \n",
    "    # make the agent and memory buffer using the parameters\n",
    "    memoryBuffer = MemoryBuffer(params['memory_buffer'])\n",
    "    agent = Agent(num_moves, params['eps_start'], params['eps_end'], params['eps_decay'], memoryBuffer, params['batch_size'], params['learning_rate'], params['ams_grad'], params['gamma'], params['targetnet_update_rate'])\n",
    "\n",
    "    # for the results of this episode (and set of parameters)\n",
    "    idx += 1\n",
    "    RewardsList = []\n",
    "    tempReward = []\n",
    "    time_to_convergence = None\n",
    "    best_average = 0\n",
    "    best_episode = None\n",
    "\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        agent.episode += 1\n",
    "\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0) \n",
    "\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            # agent interacts with the environment\n",
    "            action = agent.select_action(state)    \n",
    "            next_state, reward, terminal = env.step(action.item()) \n",
    "            \n",
    "            # turn everything into tensors here, before putting in memory\n",
    "            reward = torch.tensor([reward], device=device)\n",
    "            if not terminal:\n",
    "                next_state = torch.tensor(next_state, dtype=torch.float32, device=device).permute(2, 0, 1).unsqueeze(0)\n",
    "            \n",
    "            if terminal:\n",
    "                next_state = None\n",
    "                tempReward.append(reward.item())   \n",
    "                            \n",
    "\n",
    "            # add trajectory to memory buffer and move to the next state\n",
    "            agent.memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            # optimise the DQN model\n",
    "            agent.optimize_model()\n",
    "\n",
    "        # testing of the agent between 10 episode blocks\n",
    "        if episode % 10 == 0 and episode > 0:\n",
    "            # store the rewards of the last 10 training episode (NO SEPERATE TESTING HERE, SAVES SOME RUNNING TIME)\n",
    "            RewardsList.append(sum(tempReward)/len(tempReward))\n",
    "            tempReward = []\n",
    "\n",
    "        # find the best average over 100 episodes\n",
    "        running_avg = sum(RewardsList[-10:]) / 10   # each element in RewardsList is an average of 10 episodes\n",
    "        if running_avg > best_average:\n",
    "            best_average = running_avg\n",
    "            best_episode = episode\n",
    "        \n",
    "\n",
    "        # early stopping\n",
    "        # if the average of the previous 100 episodes was above 0.9, we've probably hit convergence\n",
    "        if episode % 10 == 0 and running_avg > 0.9:\n",
    "            time_to_convergence = episode\n",
    "            break\n",
    "\n",
    "\n",
    "    # store the results in a dataframe, making a new row for this trial here\n",
    "    tempDict = {\"DQN_model\" : DQN_model,\n",
    "                \"avgRewards\" : RewardsList,\n",
    "                \"average_last_100_episodes\" : running_avg,\n",
    "                \"best_average_100_episodes\" : best_average,\n",
    "                \"time_of_peak\" : best_episode,\n",
    "                \"time_to_convergence\" : time_to_convergence}\n",
    "    \n",
    "    resultsDict = {**params.copy(), **tempDict}  # make a line for in the results dict\n",
    "    #print(resultsDict)\n",
    "\n",
    "    RESULTS_DATAFRAME.loc[idx] = resultsDict\n",
    "\n",
    "    RESULTS_DATAFRAME.to_excel(\"Grid_search_0.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
