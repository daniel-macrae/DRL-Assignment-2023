{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "\n",
    "\n",
    "n_possible_actions = CatchEnv().get_num_actions\n",
    "# the network should have 3 outputs, one for each of the possible moves\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This is broken as fuck, do not trust it, I copied it from somewhere :/\n",
    "probably best to start over on this DQN model\n",
    "\"\"\"\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 16, 3)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)   ## THE INPUT SHAPE TO THIS IS OFF (but I can't be bothered to calculate what it should be)\n",
    "        self.fc2 = torch.nn.Linear(16 * 10 * 10, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to train the model(s)?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DQN.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\Desktop\\FSE 22-23\\Deep Reinforcement Learning\\DRL-Assignment-2023\\figuringOut.ipynb Cell 7\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/danie/Desktop/FSE%2022-23/Deep%20Reinforcement%20Learning/DRL-Assignment-2023/figuringOut.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m DQN(\u001b[39m3\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: DQN.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "\n",
    "# initialising the target and policy networks:\n",
    "\"\"\"\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 1\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n"
     ]
    }
   ],
   "source": [
    "memoryBuffer = MemoryBuffer(100)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 500\n",
    "else:\n",
    "    num_episodes = 10\n",
    "\n",
    "env = CatchEnv()\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.squeeze(state)\n",
    "    #state, reward, terminal = env.step(random.randint(0, 2))\n",
    "    terminal = False\n",
    "    while not terminal:\n",
    "\n",
    "        # agent interacts with the environment\n",
    "        action = random.randint (0, 2) # CHANGE, using the DQN model and 'state' to actually predict an action\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        next_state = np.squeeze(next_state)\n",
    "        \n",
    "\n",
    "        if terminal:\n",
    "            next_state = None\n",
    "\n",
    "        # add trajectory to memory buffer\n",
    "        memoryBuffer.push(state, action, next_state, reward)\n",
    "\n",
    "        # move onto the next state\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "        # OPTIMISE THE NETWORK HERE\n",
    "        \"\"\"\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #print (\"  Reward obtained by the agent : {}\". format(reward))\n",
    "        \n",
    "        #print(reward)\n",
    "        #plt.imshow(env.image)\n",
    "        #plt.show()\n",
    "\n",
    "    print (\"End of the episode\")\n",
    "    print (\"  Reward obtained by the agent : {}\". format(reward))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
