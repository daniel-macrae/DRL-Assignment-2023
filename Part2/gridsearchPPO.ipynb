{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mimport\u001b[39;00m A2C, PPO, TD3 \u001b[39m# these are the algorithms (models) we can use\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmonitor\u001b[39;00m \u001b[39mimport\u001b[39;00m Monitor\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39menv_util\u001b[39;00m \u001b[39mimport\u001b[39;00m make_vec_env\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "## TRAINING FILE\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from stable_baselines3 import A2C, PPO, TD3 # these are the algorithms (models) we can use\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "\n",
    "from Callbacks import SaveOnBestTrainingRewardCallback\n",
    "\n",
    "\n",
    "env_name = \"CliffWalking-v0\"\n",
    "modelName = \"PPO_Bipedal_1\"\n",
    "\n",
    "\n",
    "###   TRAINING UTILS  ###\n",
    "# directory to save the log files in\n",
    "# Logs will be saved in log_dir/modelName.csv\n",
    "log_dir = \"tmp/gridsearch/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "results_filename = log_dir + modelName + \"_\"\n",
    "# this will save the best model during training\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir, file_name=modelName)\n",
    "\n",
    "\n",
    "### ENVIRONMENT ###\n",
    "# Create and wrap the environment\n",
    "\n",
    "vec_env = make_vec_env(env_name, n_envs=16)\n",
    "vec_env = VecMonitor(vec_env, results_filename)  # this is the monitor, that saves the training episode results to the csv file\n",
    "\n",
    "\n",
    "### MAKE THE MODEL  ###\n",
    "model = PPO('MlpPolicy', vec_env, verbose=0,\n",
    "            n_steps = 2048,\n",
    "            batch_size = 64,\n",
    "            gae_lambda= 0.95,\n",
    "            gamma= 0.999,\n",
    "            n_epochs= 10,\n",
    "            ent_coef= 0.0,\n",
    "            learning_rate= 3e-4,\n",
    "            clip_range= 0.18,\n",
    "        )\n",
    "\n",
    "\n",
    "### TRAINING ###\n",
    "\n",
    "timesteps = 5e6\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRIDSEARCH STUFF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_steps': [1024, 2048, 4096],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'gae_lambda': [0.9, 0.95, 0.99],\n",
    "    'gamma': [0.99, 0.999, 0.995],\n",
    "    'n_epochs': [5, 10, 20],\n",
    "    'ent_coef': [0.0, 0.0, 0.1],\n",
    "    'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "    'clip_range': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "# Create the PPO model\n",
    "model = PPO('MlpPolicy', env)\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "\n",
    "# Fit the grid search object to perform the search\n",
    "grid_search.fit(env)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "print(\"Best Score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "from stable_baselines3 import A2C, PPO, TD3\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import VecMonitor\n",
    "from Callbacks import SaveOnBestTrainingRewardCallback\n",
    "\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "modelName = \"PPO_Bipedal_1\"\n",
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "results_filename = log_dir + modelName + \"_\"\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=100, log_dir=log_dir, file_name=modelName)\n",
    "\n",
    "# Define the parameter combinations to test\n",
    "n_steps_values = [1024, 2048, 4096]\n",
    "batch_size_values = [32, 64, 128]\n",
    "gae_lambda_values = [0.9, 0.95, 0.99]\n",
    "gamma_values = [0.99, 0.995, 0.999]\n",
    "n_epochs_values = [5, 10, 20]\n",
    "ent_coef_values = [0.0, 0.01, 0.1]\n",
    "learning_rate_values = [1e-4, 3e-4, 1e-3]\n",
    "clip_range_values = [0.1, 0.5, 0.3]\n",
    "\n",
    "# Iterate over all parameter combinations\n",
    "for n_steps in n_steps_values:\n",
    "    for batch_size in batch_size_values:\n",
    "        for gae_lambda in gae_lambda_values:\n",
    "            for gamma in gamma_values:\n",
    "                for n_epochs in n_epochs_values:\n",
    "                    for ent_coef in ent_coef_values:\n",
    "                        for learning_rate in learning_rate_values:\n",
    "                            for clip_range in clip_range_values:\n",
    "                                vec_env = make_vec_env(env_name, n_envs=16)\n",
    "                                vec_env = VecMonitor(vec_env, results_filename)\n",
    "\n",
    "                                model = PPO('MlpPolicy', vec_env, verbose=0,\n",
    "                                            n_steps=n_steps,\n",
    "                                            batch_size=batch_size,\n",
    "                                            gae_lambda=gae_lambda,\n",
    "                                            gamma=gamma,\n",
    "                                            n_epochs=n_epochs,\n",
    "                                            ent_coef=ent_coef,\n",
    "                                            learning_rate=learning_rate,\n",
    "                                            clip_range=clip_range)\n",
    "\n",
    "                                timesteps = 5e6\n",
    "                                model.learn(total_timesteps=int(timesteps), callback=callback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...\n",
    "# After the grid search loop\n",
    "\n",
    "# Extract the rewards from the log files\n",
    "rewards = []\n",
    "for n_steps in n_steps_values:\n",
    "    for batch_size in batch_size_values:\n",
    "        for gae_lambda in gae_lambda_values:\n",
    "            for gamma in gamma_values:\n",
    "                for n_epochs in n_epochs_values:\n",
    "                    for ent_coef in ent_coef_values:\n",
    "                        for learning_rate in learning_rate_values:\n",
    "                            for clip_range in clip_range_values:\n",
    "                                log_file = log_dir + modelName + \"_\" + f\"n_steps={n_steps}_batch_size={batch_size}_gae_lambda={gae_lambda}_gamma={gamma}_n_epochs={n_epochs}_ent_coef={ent_coef}_learning_rate={learning_rate}_clip_range={clip_range}.csv\"\n",
    "                                rewards.append(np.loadtxt(log_file, delimiter=\",\", skiprows=1, usecols=1)[-1])\n",
    "\n",
    "# Find the index of the best reward\n",
    "best_reward_index = np.argmax(rewards)\n",
    "\n",
    "# Retrieve the corresponding best parameter combination\n",
    "best_n_steps = n_steps_values[best_reward_index]\n",
    "best_batch_size = batch_size_values[best_reward_index]\n",
    "best_gae_lambda = gae_lambda_values[best_reward_index]\n",
    "best_gamma = gamma_values[best_reward_index]\n",
    "best_n_epochs = n_epochs_values[best_reward_index]\n",
    "best_ent_coef = ent_coef_values[best_reward_index]\n",
    "best_learning_rate = learning_rate_values[best_reward_index]\n",
    "best_clip_range = clip_range_values[best_reward_index]\n",
    "\n",
    "# Print the best parameter combination and its corresponding reward\n",
    "print(\"Best Parameter Combination:\")\n",
    "print(f\"n_steps: {best_n_steps}\")\n",
    "print(f\"batch_size: {best_batch_size}\")\n",
    "print(f\"gae_lambda: {best_gae_lambda}\")\n",
    "print(f\"gamma: {best_gamma}\")\n",
    "print(f\"n_epochs: {best_n_epochs}\")\n",
    "print(f\"ent_coef: {best_ent_coef}\")\n",
    "print(f\"learning_rate: {best_learning_rate}\")\n",
    "print(f\"clip_range: {best_clip_range}\")\n",
    "print(f\"Best Reward: {rewards[best_reward_index]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
