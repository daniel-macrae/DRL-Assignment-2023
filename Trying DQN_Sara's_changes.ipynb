{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the network should have 3 outputs, one for each of the possible moves\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# I CHANGED A BUNCH OF STUFF\n",
    "\"\"\"\n",
    "This is broken as fuck, do not trust it, I copied it from somewhere :/\n",
    "probably best to start over on this DQN model\n",
    "\"\"\"\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 8, stride=4)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 64, 3)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, eps_start, eps_min, eps_decay, memory, batch_size, optimizer, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = memory\n",
    "        self.env = env\n",
    "        self.eps = eps_start\n",
    "        self.eps = eps_min\n",
    "        self.eps = eps_decay\n",
    "\n",
    "        self.model = DQN(env.get_num_actions()).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "\n",
    "    def select_action(self, state):              #NOT REALLY SURE IF THIS IS DOING WHAT IT NEEDS TO\n",
    "        if np.random.rand() <= self.eps:\n",
    "            return random.randrange(self.env.get_num_actions())\n",
    "\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        \n",
    "        print(state.shape)\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        q_values = self.model(state)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        return action\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).unsqueeze(1).to(device) # maybe take out the unsqueeze\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "        terminals = torch.from_numpy(np.array(terminals)).float().to(device)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - terminals) * self.gamma * next_q_values\n",
    "\n",
    "        loss = F.mse_loss(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps *= self.eps_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "\n",
    "# initialising the target and policy networks:\n",
    "env = CatchEnv()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.get_num_actions()\n",
    "\n",
    "# Get the number of state observations\n",
    "env.reset()\n",
    "state, reward, terminal = env.step(random.randint(0, 2))\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_network = DQN(n_actions).to(device)\n",
    "target_network = DQN(n_actions).to(device)\n",
    "\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "criterion = nn.MSELoss() # this one is inside the agent I think\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "memoryBuffer = MemoryBuffer(100)\n",
    "\n",
    "\n",
    "agent = Agent(env, EPS_START, EPS_END, EPS_DECAY, memoryBuffer, BATCH_SIZE, optimizer, GAMMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of the episode\n",
      "  Reward obtained by the agent : 0\n",
      "(1, 84, 84, 4)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [32, 4, 8, 8], expected input[1, 84, 84, 4] to have 4 channels, but got 84 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2b\\Deep Reinforcement Learning\\Assignments\\DRL-Assignment-2023\\Trying DQN_Sara's_changes.ipynb Cell 10\u001b[0m in \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# agent interacts with the environment\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mselect_action(state)    \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     next_state, reward, terminal \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     next_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(next_state)\n",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2b\\Deep Reinforcement Learning\\Assignments\\DRL-Assignment-2023\\Trying DQN_Sara's_changes.ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(state\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(state)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(q_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Usuario\\Documents\\Gronningen\\Uni\\Year 1\\Semester 2b\\Deep Reinforcement Learning\\Assignments\\DRL-Assignment-2023\\Trying DQN_Sara's_changes.ipynb Cell 10\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Max pooling over a (2, 2) window\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmax_pool2d(x, (\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Documents/Gronningen/Uni/Year%201/Semester%202b/Deep%20Reinforcement%20Learning/Assignments/DRL-Assignment-2023/Trying%20DQN_Sara%27s_changes.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [32, 4, 8, 8], expected input[1, 84, 84, 4] to have 4 channels, but got 84 channels instead"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 500\n",
    "else:\n",
    "    num_episodes = 10\n",
    "\n",
    "env = CatchEnv()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.squeeze(state)\n",
    "    #state, reward, terminal = env.step(random.randint(0, 2))\n",
    "    terminal = False\n",
    "    total_reward = 0\n",
    "    while not terminal:\n",
    "        # agent interacts with the environment\n",
    "        action = agent.select_action(state)    \n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        next_state = np.squeeze(next_state)\n",
    "        \n",
    "        if terminal:\n",
    "            next_state = None\n",
    "\n",
    "        # add trajectory to memory buffer\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "        # move onto the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        agent.replay()\n",
    "            \n",
    "        # Update the target network\n",
    "        if episode % 10 == 0: ## MAYBE USE THE AGENTS FUNCTION\n",
    "            target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "    print (\"End of the episode\")\n",
    "    print (\"  Reward obtained by the agent : {}\". format(total_reward))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arquitectures for the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
