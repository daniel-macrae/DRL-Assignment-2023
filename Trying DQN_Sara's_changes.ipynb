{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\miniconda3\\envs\\footballtracking\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from skimage.transform import resize\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Catch Environment (from the assignment code)\n",
    "from Catch import CatchEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory buffer, to store the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity) # iterable deque data structure\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the network should have 3 outputs, one for each of the possible moves\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# I CHANGED A BUNCH OF STUFF\n",
    "\"\"\"\n",
    "This is broken as fuck, do not trust it, I copied it from somewhere :/\n",
    "probably best to start over on this DQN model\n",
    "\"\"\"\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, number_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # conv layers\n",
    "        self.conv1 = torch.nn.Conv2d(4, 32, 8, stride=4)   # modify input shape to match your input data size\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(64, 64, 3)\n",
    "        # fully connected layers\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, number_of_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, env, eps_start, eps_min, eps_decay, memory, batch_size, optimizer, gamma):\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = memory\n",
    "        self.env = env\n",
    "        self.eps = eps_start\n",
    "        self.eps = eps_min\n",
    "        self.eps = eps_decay\n",
    "\n",
    "        self.model = DQN(env.get_num_actions()).to(device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "\n",
    "    def select_action(self, state):              #NOT REALLY SURE IF THIS IS DOING WHAT IT NEEDS TO\n",
    "        if np.random.rand() <= self.eps:\n",
    "            return random.randrange(self.env.get_num_actions())\n",
    "\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        \n",
    "        print(state.shape)\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        q_values = self.model(state)\n",
    "        action = torch.argmax(q_values, dim=1).item()\n",
    "        return action\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).unsqueeze(1).to(device) # maybe take out the unsqueeze\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "        terminals = torch.from_numpy(np.array(terminals)).float().to(device)\n",
    "\n",
    "        q_values = self.model(states)\n",
    "        q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.model(next_states).max(1)[0]\n",
    "        expected_q_values = rewards + (1 - terminals) * self.gamma * next_q_values\n",
    "\n",
    "        loss = F.mse_loss(q_values, expected_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.eps > self.eps_min:\n",
    "            self.eps *= self.eps_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_state_dict(torch.load(name))\n",
    "\n",
    "    def save(self, name):\n",
    "        torch.save(self.model.state_dict(), name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# BATCH_SIZE is the number of transitions sampled from the replay buffer\\n# GAMMA is the discount factor as mentioned in the previous section\\n# EPS_START is the starting value of epsilon\\n# EPS_END is the final value of epsilon\\n# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\\n# TAU is the update rate of the target network\\n# LR is the learning rate of the ``AdamW`` optimizer\\nBATCH_SIZE = 128\\nGAMMA = 0.99\\nEPS_START = 0.9\\nEPS_END = 0.05\\nEPS_DECAY = 1000\\nTAU = 0.005\\nLR = 1e-4\\n\\n\\n# initialising the target and policy networks:\\nenv = CatchEnv()\\n\\n# Get number of actions from gym action space\\nn_actions = env.get_num_actions()\\n\\n# Get the number of state observations\\nenv.reset()\\nstate, reward, terminal = env.step(random.randint(0, 2))\\nn_observations = len(state)\\n\\npolicy_network = DQN(n_actions).to(device)\\ntarget_network = DQN(n_actions).to(device)\\n\\ntarget_network.load_state_dict(policy_network.state_dict())\\n\\ncriterion = nn.MSELoss() # this one is inside the agent I think\\noptimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\\n\\nmemoryBuffer = MemoryBuffer(100)\\n\\n\\nagent = Agent(env, EPS_START, EPS_END, EPS_DECAY, memoryBuffer, BATCH_SIZE, optimizer, GAMMA)\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "# GAMMA is the discount factor as mentioned in the previous section\n",
    "# EPS_START is the starting value of epsilon\n",
    "# EPS_END is the final value of epsilon\n",
    "# EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "# TAU is the update rate of the target network\n",
    "# LR is the learning rate of the ``AdamW`` optimizer\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "\n",
    "# initialising the target and policy networks:\n",
    "env = CatchEnv()\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = env.get_num_actions()\n",
    "\n",
    "# Get the number of state observations\n",
    "env.reset()\n",
    "state, reward, terminal = env.step(random.randint(0, 2))\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_network = DQN(n_actions).to(device)\n",
    "target_network = DQN(n_actions).to(device)\n",
    "\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "criterion = nn.MSELoss() # this one is inside the agent I think\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "\n",
    "memoryBuffer = MemoryBuffer(100)\n",
    "\n",
    "\n",
    "agent = Agent(env, EPS_START, EPS_END, EPS_DECAY, memoryBuffer, BATCH_SIZE, optimizer, GAMMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 4)\n",
      "doing conv1\n",
      "torch.Size([1, 84, 84, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[0;32m     33\u001b[0m     \u001b[39m# agent interacts with the environment\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[39mprint\u001b[39m(state\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 35\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mselect_action(state)    \n\u001b[0;32m     36\u001b[0m     \u001b[39m#print(action)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m     next_state, reward, terminal \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[34], line 27\u001b[0m, in \u001b[0;36mAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m#print(state.shape)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(state)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 27\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(state)\n\u001b[0;32m     28\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(q_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mc:\\Users\\danie\\miniconda3\\envs\\footballtracking\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[33], line 28\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdoing conv1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 28\u001b[0m x\u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mpermute(\u001b[39m2\u001b[39;49m, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     29\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     30\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 5 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 500\n",
    "else:\n",
    "    num_episodes = 10\n",
    "\n",
    "env = CatchEnv()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.squeeze(state)\n",
    "    #state, reward, terminal = env.step(random.randint(0, 2))\n",
    "    terminal = False\n",
    "    total_reward = 0\n",
    "    while not terminal:\n",
    "        # agent interacts with the environment\n",
    "        action = agent.select_action(state)    \n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        next_state = np.squeeze(next_state)\n",
    "        \n",
    "        if terminal:\n",
    "            next_state = None\n",
    "\n",
    "        # add trajectory to memory buffer\n",
    "        agent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "        # move onto the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Sample a batch of transitions from the replay buffer\n",
    "        agent.replay()\n",
    "            \n",
    "        # Update the target network\n",
    "        if episode % 10 == 0: ## MAYBE USE THE AGENTS FUNCTION\n",
    "            target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "\n",
    "\n",
    "    print (\"End of the episode\")\n",
    "    print (\"  Reward obtained by the agent : {}\". format(total_reward))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "arquitectures for the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
